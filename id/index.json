[{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"Complete Post Coming Soon\u0026hellip; ","date":"March 18, 2022","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/3_kubernetes/artikel-2/artikel-2/","summary":"Complete Post Coming Soon\u0026hellip; ","tags":null,"title":"Clustering Kubernetes using Kubeadm"},{"categories":["Monitoring","Linux","Node-Exporter","Prometheus","Grafana"],"contents":"Halo Gaes, Pada jurnal kali ini kita akan coba untuk menginstall Node Exporter pada Ubuntu dan mem-visualisasikan hasil dari metric Node Exporter tersebut kedalam Dashboard Grafana. Secara garis besar Node Exporter disini berfungsi sebagai agent untuk mengubah data-data pada server menjadi metric, kemudian metrics tersebut kita lempar ke Prometheus dan di visualisasikan metric itu kedalam Dashboard Grafana agar lebih mudah dalam membacanya. Untuk lab kali ini hal-hal yang akan kita lakukan yaitu:\n Install Node Exporter di setiap VM/Instances Install Prometheus disalah satu VM/Instances Install dan configure Grafana Import Dashboard Grafana  Pada lab ini kita masih menggunakan environtment dari lab yang sebelumnya, jadi untuk topologi yang digunakan masih sama. Install dan konfigurasi Prometheus dan Grafana dilakukan di VM rb-k8s-lb1\nInstall Node Exporter di setiap VM/Instances Okay, kita langsung saja ke step pertama yaitu install node exporter di semua VM/instances, kenapa? karena tujuan kita yaitu untuk memonitor resources semua VM tersebut, skuyy!\n Langkah yang pertama harus kita lakukan yaitu install Node exporternya dengan command:  wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz Setelah kita download kemudian kita ekstrak file tersebut dengan command:  tar xvfz node_exporter-1.3.1.linux-amd64.tar.gz Kemudian untuk memudahkan kita dalam proses intallasi karena kita tentu tidak akan melakukan install paket node exporter tersebut secara manual satu persatu, kita bisa lakukan menggunakan script. Untuk script pertama kita siapkan daftar list IP VM mana saja yang akan kita install node exporter. kita beri file dengan nama ip-list.txt  sudo nano ip-list.txt 10.60.60.43 10.60.60.44 10.60.60.51 10.60.60.52 10.60.60.53 10.60.60.54 10.60.60.55 10.60.60.56 Lalu kita bikin script untuk install node_exporter tadi, kita namakan script ini add-node-exporter.sh  sudo nano add-node-exporter.sh Kemudian kita bikin service node_exporter dengan nama node_exporter.service  sudo nano node_exporter.service [Unit] Description=Node Exporter After=network.target [Service] User=root Group=root Type=simple ExecStart=/usr/local/bin/node_exporter [Install] WantedBy=multi-user.target Sampai sini kita harusnya sudah mempunyai 3 file, yaitu add-node-exporter.sh node_exporter.service dan add-node-exporter.sh. Kemudian kita bikin script untuk mentransfer file semua script tadi ke masing-masing VM. Kita beri nama file ini dengan nama transfer-node-exporter.sh  sudo nano transfer-node-exporter.sh #!/bin/bash for i in $(cat ip-list.txt) do echo \u0026quot;#### Execute On $i ####\u0026quot; scp node_exporter node_exporter.service add-node-exporter.sh $i:~/ done Setelah file siap. kita tinggal jalankan script transfer-node-exporter.sh dengan cara:  sudo chmod +x transfer-node-exporter.sh sudo ./transfer-node-exporter.sh Jika semua file telah di transfer ke masing-masing VM, kita siapkan satu script lagi untuk install node exporternya. kita namakan file ini dengan install-node-exporter.sh  sudo nano install-node-exporter.sh #!/bin/bash for i in $(cat ip-list.txt) do echo \u0026quot;#### Installing On $i ####\u0026quot; ssh $i bash add-node-exporter.sh systemctl status node_exporter.service done Untuk mulai install node exporter di semua VM, kita cukup menggunakan file yang terakhir saja karena file sebelumnya kita sudah copy ke masing-masing VM:  sudo chmod +x install-node-exporter.sh ./install-node-exporter.sh Apabila kita ingin menginstal node exporter di kemudian hari dengan jumlah VM yang lumayan banyak, kita tinggal rubah saja untuk list IPnya pada file IP-list.txt, kemudian kita transfer semua file menggunakan script transfer-node-exporter.sh lalu kita bisa jalankan script install-node-exporter.sh untuk langsung mulai install di semua VM.\nVerifikasi Untuk memverifikasi hasil install node-exporter dapat kita lakukan dengan menggunakan command:\nroot@rb-k8s-lb1:~# sudo systemctl status node_exporter.service ● node_exporter.service - Node Exporter Loaded: loaded (/etc/systemd/system/node_exporter.service; disabled; vendor preset: enabled) Active: active (running) since Wed 2022-02-02 16:24:00 UTC; 1 months 6 days ago Main PID: 2006 (node_exporter) Tasks: 10 (limit: 4676) Memory: 22.4M CGroup: /system.slice/node_exporter.service └─2006 /usr/local/bin/node_exporter Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=thermal_zone Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=time Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=timex Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=udp_queues Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=uname Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=vmstat Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=xfs Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.253Z caller=node_exporter.go:115 level=info collector=zfs Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.254Z caller=node_exporter.go:199 level=info msg=\u0026quot;Listening on\u0026quot; address=:9100 Feb 02 16:24:00 rb-k8s-lb1 node_exporter[2006]: ts=2022-02-02T16:24:00.254Z caller=tls_config.go:195 level=info msg=\u0026quot;TLS is disabled.\u0026quot; http2=false root@rb-k8s-lb1:~# atau bisa menggunakan perintah curl melalui terminal\nroot@rb-k8s-lb1:~# curl localhost:9100 -k ","date":"February 18, 2022","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/5_monitoring/monitoring-vm-1-node-exporter/monitoring-vm-1-node-exporter/","summary":"Halo Gaes, Pada jurnal kali ini kita akan coba untuk menginstall Node Exporter pada Ubuntu dan mem-visualisasikan hasil dari metric Node Exporter tersebut kedalam Dashboard Grafana. Secara garis besar Node Exporter disini berfungsi sebagai agent untuk mengubah data-data pada server menjadi metric, kemudian metrics tersebut kita lempar ke Prometheus dan di visualisasikan metric itu kedalam Dashboard Grafana agar lebih mudah dalam membacanya. Untuk lab kali ini hal-hal yang akan kita lakukan yaitu:","tags":["Kubernetes","Linux"],"title":"Cara Mudah Monitoring VM Ubuntu Menggunakan Node Exporter, Prometheus dan Grafana"},{"categories":["Monitoring","Linux","Node-Exporter","Prometheus","Grafana"],"contents":"Hai Gaes! Pada jurnal sebelumnya kita sudah bahas mengenai Cara Mudah Monitoring VM Ubuntu Menggunakan Node Exporter, Prometheus dan Grafana bagian satu yaitu install node exporter nih, sekarang kita lanjut buat install Prometheus dan Grafananya skuy! Environtment dan resources masih sama seperti yang sebelumnya yah, kali ini kita lanjutin labnya.\nOke, karena kita masih menggunakan topologi yang sama kita bisa langsung lanjut aja ke cara Install Prometheus disalah satu VM/Instances. Ingat kita install Prometheus di VM rb-k8s-lb1 ya\nInstall dan konfigurasi  Langkah pertama kita download file Prometheusnya dulu dengan command:  wget https://github.com/prometheus/prometheus/releases/download/v2.33.0/prometheus-2.33.0.linux-amd64.tar.gz Kemudian kita ekstrak hasil download tadi:  tar xvfz prometheus-2.33.0.linux-amd64.tar.gz Kita pindahkan hasil ekstrak ke directory /opt  mv prometheus-2.33.0.linux-amd64 /opt Kemudian ubah isi file prometheus.yaml menjadi seperti ini:  sudo cp /opt/prometheus-2.33.0.linux-amd64/prometheus.yml /opt/prometheus-2.33.0.linux-amd64/prometheus.yml.bak sudo nano /opt/prometheus-2.33.0.linux-amd64/prometheus.yml # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \u0026quot;first_rules.yml\u0026quot; # - \u0026quot;second_rules.yml\u0026quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=job_name` to any timeseries scraped from this config. - job_name: 'loadbalancer' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: - '10.60.60.43:9100' - '10.60.60.44:9100' - job_name: 'master' static_configs: - targets: - '10.60.60.51:9100' - '10.60.60.52:9100' - '10.60.60.53:9100' - job_name: 'worker' static_configs: - targets: - '10.60.60.54:9100' - '10.60.60.55:9100' - '10.60.60.56:9100' Buat file dengan nama prometheus_server.service agar ketika server di reboot/restart service tetap berjalan  nano /etc/systemd/system/prometheus_server.service [Unit] Description=Prometheus Server [Service] User=root ExecStart=/opt/prometheus-2.33.0.linux-amd64/prometheus --config.file=/opt/prometheus-2.33.0.linux-amd64/prometheus.yml --web.external-url=http://10.60.60.43:9090/ [Install] WantedBy=default.target Kemudian Enable, start, Service tersebut dengan command:  systemctl enable prometheus_server.service systemctl start prometheus_server.service\\ Verifikasi Pastikan service Prometheus berjalan  systemctl status prometheus_server.service root@rb-k8s-lb1:/opt/prometheus-2.33.0.linux-amd64# systemctl status prometheus_server.service ● prometheus_server.service - Prometheus Server Loaded: loaded (/etc/systemd/system/prometheus_server.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2022-03-11 18:59:53 UTC; 2s ago Main PID: 1408527 (prometheus) Tasks: 10 (limit: 4676) Memory: 70.7M CGroup: /system.slice/prometheus_server.service └─1408527 /opt/prometheus-2.33.0.linux-amd64/prometheus --config.file=/opt/prometheus-2.33.0.linux-amd64/prometheus.yml --web.external-url=http://10.60.60.43:9090/ Mar 11 18:59:54 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:54.952Z caller=head.go:604 level=info component=tsdb msg=\u0026quot;WAL segment loaded\u0026quot; segment=371 maxSegment=374 Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.148Z caller=head.go:604 level=info component=tsdb msg=\u0026quot;WAL segment loaded\u0026quot; segment=372 maxSegment=374 Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.340Z caller=head.go:604 level=info component=tsdb msg=\u0026quot;WAL segment loaded\u0026quot; segment=373 maxSegment=374 Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.341Z caller=head.go:604 level=info component=tsdb msg=\u0026quot;WAL segment loaded\u0026quot; segment=374 maxSegment=374 Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.341Z caller=head.go:610 level=info component=tsdb msg=\u0026quot;WAL replay completed\u0026quot; checkpoint_replay_duration=1.203064377s wal_replay_duration=534.227604ms total_replay_duration=1.776991108s Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.346Z caller=main.go:944 level=info fs_type=EXT4_SUPER_MAGIC Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.346Z caller=main.go:947 level=info msg=\u0026quot;TSDB started\u0026quot; Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.346Z caller=main.go:1128 level=info msg=\u0026quot;Loading configuration file\u0026quot; filename=/opt/prometheus-2.33.0.linux-amd64/prometheus.yml Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.357Z caller=main.go:1165 level=info msg=\u0026quot;Completed loading of configuration file\u0026quot; filename=/opt/prometheus-2.33.0.linux-amd64/prometheus.yml totalDuration=10.632798ms db_storage=1.542µs remote_stora\u0026gt; Mar 11 18:59:55 rb-k8s-lb1 prometheus[1408527]: ts=2022-03-11T18:59:55.357Z caller=main.go:896 level=info msg=\u0026quot;Server is ready to receive web requests.\u0026quot; root@rb-k8s-lb1:/opt/prometheus-2.33.0.linux-amd64# Untuk testing bisa gunakan browser atau dengan command curl melalui port 9090\nroot@rb-k8s-lb1:/opt/prometheus-2.33.0.linux-amd64# curl 10.60.60.43:9090 Ok gaes, untuk kali ini sampai sini dulu, nanti kita lanjut untuk install Grafananya yah, terimakasih\n","date":"February 18, 2022","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/5_monitoring/monitoring-vm-2-prometheus/monitoring-vm-prometheus/","summary":"Hai Gaes! Pada jurnal sebelumnya kita sudah bahas mengenai Cara Mudah Monitoring VM Ubuntu Menggunakan Node Exporter, Prometheus dan Grafana bagian satu yaitu install node exporter nih, sekarang kita lanjut buat install Prometheus dan Grafananya skuy! Environtment dan resources masih sama seperti yang sebelumnya yah, kali ini kita lanjutin labnya.\nOke, karena kita masih menggunakan topologi yang sama kita bisa langsung lanjut aja ke cara Install Prometheus disalah satu VM/Instances. Ingat kita install Prometheus di VM rb-k8s-lb1 ya","tags":["Kubernetes","Linux"],"title":"Cara Mudah Monitoring VM Ubuntu Menggunakan Node Exporter, Prometheus dan Grafana - Bagian II"},{"categories":["Kubernetes","Linux"],"contents":"Pada kesempatan kali ini saya akan membagikan bagaimana cara konfigurasi High Availability Cluster Kubernetes di Ubuntu menggunakan HAProxy dan Keepalived. Kita menggunakan Keepalived dan HAproxy untuk load balancing dan high availabilty-nya. Hal yang perlu di persiapkan antara lain:\n Kebutuhan hosts konfigurasi HAproxy dan Keepalived Konfigurasi clustering Kubernetes menggunakan Kubeadm.  Architecture Clusters. Disini kita menggunakan 3 master, 3 worker, 2 loadbalancer, dan 1 IP untuk digunakan sebagai Virtual IP Address. VIP artinya jika terjadi error atau 1 node mati IP dapat di teruskan ke node yang memungkinkan terjadinya flaiover, sehingga tercapai High Availability sesuai dengan tujuan lab kita saat ini.\n Perlu digaris bawahi, bahwa HAproxy dan Keepalived tidak di install di node master. HAproxy dan Keepalived hanya di install di node loadbalancer1 dan loadbalancer2\nHost yang digunakan:\n   Hostname IP Address Roles     rb-k8s-lb1 10.60.60.43 HAproxy \u0026amp; Keepalived   rb-k8s-lb2 10.60.60.44 HAproxy \u0026amp; Keepalived   rb-k8s-master1 10.60.60.51 master, etcd   rb-k8s-master2 10.60.60.52 master, etcd   rb-k8s-master3 10.60.60.53 master, etcd   rb-k8s-worker1 10.60.60.54 worker   rb-k8s-worker2 10.60.60.55 worker   rb-k8s-worker3 10.60.60.56 worker   - 10.60.60.45 VirtualIP    Konfigurasi Load Balancing: Keepalived dan HAproxy di install pada node rb-k8s-lb1 dan rb-k8s-lb2. Dengan sekenario jika salah satu node loadbalancer ada yang down/mati maka Virtual IP akan otomatis menggunakan jalur di node loadbalancer yang sedang running sehingga cluster kubernetes tidak mengalami kegagalan/error.\nInstall Keepalived dan HAproxy pada kedua node  Jalankan pada node rb-k8s-lb1 dan rb-k8s-lb2:  sudo apt install keepalived haproxy psmisc -y Lakukan konfigurasi pada haproxy di kedua node loadbalancer:  sudo vi /etc/haproxy/haproxy.cfg global log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server rb-k8s-master1 10.60.60.51:6443 check # Replace the IP address with your own. server rb-k8s-master2 10.60.60.52:6443 check # Replace the IP address with your own. server rb-k8s-master3 10.60.60.53:6443 check # Replace the IP address with your own. Restart service load HAproxy:  sudo systemctl restart haproxy Kemudian restart dan enable service HAproxy:  sudo systemctl restart haproxy sudo systemctl enable haproxy Pastikan service HAproxy active:  root@rb-k8s-lb1:~# systemctl status haproxy.service ● haproxy.service - HAProxy Load Balancer Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2022-02-12 17:29:05 UTC; 10min ago Docs: man:haproxy(1) file:/usr/share/doc/haproxy/configuration.txt.gz Process: 463986 ExecStartPre=/usr/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS (code=exited, status=0/SUCCESS) Main PID: 464001 (haproxy) Tasks: 5 (limit: 4676) Memory: 3.6M CGroup: /system.slice/haproxy.service ├─464001 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock └─464002 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock Feb 12 17:29:05 rb-k8s-lb1 systemd[1]: Starting HAProxy Load Balancer... Feb 12 17:29:05 rb-k8s-lb1 haproxy[464001]: [WARNING] 042/172905 (464001) : parsing [/etc/haproxy/haproxy.cfg:28] : backend 'kube-apiserver' \u0026gt; Feb 12 17:29:05 rb-k8s-lb1 haproxy[464001]: [NOTICE] 042/172905 (464001) : New worker #1 (464002) forked Feb 12 17:29:05 rb-k8s-lb1 systemd[1]: Started HAProxy Load Balancer. Feb 12 17:29:08 rb-k8s-lb1 haproxy[464002]: [WARNING] 042/172908 (464002) : Server kube-apiserver/rb-k8s-master2 is DOWN, reason: Layer4 conn\u0026gt; Feb 12 17:29:12 rb-k8s-lb1 haproxy[464002]: [WARNING] 042/172912 (464002) : Server kube-apiserver/rb-k8s-master3 is DOWN, reason: Layer4 conn\u0026gt; Konfigurasi Keepalived  Lakukan konfigurasi keepalived pada kedua node rb-k8s-lb1 dan rb-k8s-lb2:  sudo /etc/keepalived/keepalived.conf Konfigurasi keepalived.conf pada rb-k8s-lb1:\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026quot;killall -0 haproxy\u0026quot; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface ens3 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.60.60.43 # The IP address of this machine rb-k8s-lb1 unicast_peer { 10.60.60.44 # The IP address of peer machines rb-k8s-lb2 } virtual_ipaddress { 10.60.60.45/24 # The VIP address } track_script { chk_haproxy } } Konfigurasi keepalived.conf pada rb-k8s-lb2:\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026quot;killall -0 haproxy\u0026quot; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface ens3 # Network card virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip 10.60.60.44 # The IP address of this machine rb-k8s-lb2 unicast_peer { 10.60.60.43 # The IP address of peer machines rb-k8s-lb1 } virtual_ipaddress { 10.60.60.45/24 # The VIP address } track_script { chk_haproxy } } Kemudian restart dan enable service keepalived  sudo systemctl restart keepalived sudo systemctl enable keepalived Pastikan service keepalived running Status keepalived pada rb-k8s-lb1:  root@rb-k8s-lb1:~# sudo systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2022-02-02 16:07:10 UTC; 1 weeks 3 days ago Main PID: 647 (keepalived) Tasks: 2 (limit: 4676) Memory: 6.5M CGroup: /system.slice/keepalived.service ├─647 /usr/sbin/keepalived --dont-fork └─708 /usr/sbin/keepalived --dont-fork Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: Registering Kernel netlink command channel Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: Opening file '/etc/keepalived/keepalived.conf'. Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: WARNING - default user 'keepalived_script' for script execution does not exist - please crea\u0026gt; Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: WARNING - script `killall` resolved by path search to `/usr/bin/killall`. Please specify ful\u0026gt; Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: Registering gratuitous ARP shared channel Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: (haproxy-vip) Entering BACKUP STATE (init) Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: VRRP_Script(chk_haproxy) succeeded Feb 02 16:07:10 rb-k8s-lb1 Keepalived_vrrp[708]: (haproxy-vip) Changing effective priority from 100 to 102 Feb 02 16:07:14 rb-k8s-lb1 Keepalived_vrrp[708]: (haproxy-vip) Entering MASTER STATE Status keepalived pada rb-k8s-lb2:\nroot@rb-k8s-lb2:~# sudo systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2022-02-02 16:07:12 UTC; 1 weeks 3 days ago Main PID: 647 (keepalived) Tasks: 2 (limit: 4676) Memory: 6.6M CGroup: /system.slice/keepalived.service ├─647 /usr/sbin/keepalived --dont-fork └─699 /usr/sbin/keepalived --dont-fork Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: Registering Kernel netlink reflector Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: Registering Kernel netlink command channel Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: Opening file '/etc/keepalived/keepalived.conf'. Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: WARNING - default user 'keepalived_script' for script execution does not exist - please crea\u0026gt; Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: WARNING - script `killall` resolved by path search to `/usr/bin/killall`. Please specify ful\u0026gt; Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: SECURITY VIOLATION - scripts are being executed but script_security not enabled. Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: Registering gratuitous ARP shared channel Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: (haproxy-vip) Entering BACKUP STATE (init) Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: VRRP_Script(chk_haproxy) succeeded Feb 02 16:07:13 rb-k8s-lb2 Keepalived_vrrp[699]: (haproxy-vip) Changing effective priority from 100 to 102 Verifikasi High Availability Sebelum kita membuat cluster kubernetes kita harus memastikan konfigurasi high availabilty kita berfungsi dengan semestinya\n Cek IP Address pada node rb-k8s-lb1:  root@rb-k8s-lb1:~# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:72:9a:4b brd ff:ff:ff:ff:ff:ff inet 10.60.60.43/24 brd 10.60.60.255 scope global ens3 valid_lft forever preferred_lft forever inet 10.60.60.45/24 scope global secondary ens3 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe72:9a4b/64 scope link valid_lft forever preferred_lft forever Bisa kita lihat bahwa VIP 10.60.60.45 berhasil ditambahkan pada node rb-k8s-lb1. kemudian kita simulasikan jika node rb-k8s-lb1 down, maka seharusnya VIP 10.60.60.45 akan otomatis berpindah ke node rb-k8s-lb2\nKita matikan service HAproxy pada node rb-k8s-lb1:  sudo systemctl stop haproxy.service Lalu kita cek lagi IP Address pada node rb-k8s-lb1:  root@rb-k8s-lb1:~# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:72:9a:4b brd ff:ff:ff:ff:ff:ff inet 10.60.60.43/24 brd 10.60.60.255 scope global ens3 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe72:9a4b/64 scope link valid_lft forever preferred_lft forever Pada node rb-k8s-lb1 VIP 10.60.60.45 sudah tidak ada, maka kita harus pastikan jika VIP berada pada node rb-k8s-lb2\nCek IP Address pada node rb-k8s-lb2:   root@rb-k8s-lb2:~# ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:aa:3d:02 brd ff:ff:ff:ff:ff:ff inet 10.60.60.44/24 brd 10.60.60.255 scope global ens3 valid_lft forever preferred_lft forever inet 10.60.60.45/24 scope global secondary ens3 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:feaa:3d02/64 scope link valid_lft forever preferred_lft forever Sesuai harapan kita bahwa VIP sudah berpindah ke node rb-k8s-lb2 jika disimulasikan node rb-k8s-lb1 mati dengan menonaktifkan service haproxy pada rb-k8s-lb1.\nJangan lupa kita start lagi service haproxy pada node rb-k82-lb1:  sudo systemctl start haproxy.service Pada postingan berikutnya kita akan melanjutkan dengan mengkonfigurasi cluster kubernetes menggunakan alamat VIP yang sudah kita buat tadi\n","date":"February 13, 2022","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/3_kubernetes/cara-konfigurasi-ha/cara-konfigurasi-ha/","summary":"Pada kesempatan kali ini saya akan membagikan bagaimana cara konfigurasi High Availability Cluster Kubernetes di Ubuntu menggunakan HAProxy dan Keepalived. Kita menggunakan Keepalived dan HAproxy untuk load balancing dan high availabilty-nya. Hal yang perlu di persiapkan antara lain:\n Kebutuhan hosts konfigurasi HAproxy dan Keepalived Konfigurasi clustering Kubernetes menggunakan Kubeadm.  Architecture Clusters. Disini kita menggunakan 3 master, 3 worker, 2 loadbalancer, dan 1 IP untuk digunakan sebagai Virtual IP Address.","tags":["Kubernetes","Linux"],"title":"Cara Konfigurasi High Availabilty Kubernetes Cluster di Ubuntu 20.4"},{"categories":["Basic"],"contents":"Greeting! This is an introduction post.\n","date":"June 8, 2020","hero":"/id/posts/1_introduction/hero.svg","permalink":"https://me.byamri.com/id/posts/1_introduction/","summary":"Greeting! This is an introduction post.","tags":["Basic","Multi-lingual"],"title":"Introduction"},{"categories":null,"contents":"Complete Post Coming Soon\u0026hellip; ","date":"March 18, 2020","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/4_docker/artikel-1/artikel-1/","summary":"Complete Post Coming Soon\u0026hellip; ","tags":null,"title":"Docker Introduction"},{"categories":null,"contents":"Complete Post Coming Soon\u0026hellip; ","date":"March 18, 2020","hero":"/images/default-hero.jpg","permalink":"https://me.byamri.com/id/posts/2_openstack/artikel-1/artikel-1/","summary":"Complete Post Coming Soon\u0026hellip; ","tags":null,"title":"Openstack Introduction"}]